syntax = "proto3";

package zkx.gpu;

// Generic backend config for XLA:GPU
// Next-Id: 12
message GpuBackendConfig {
  // Specifies which operation queue the current instruction will run on.
  // A backend may have multiple operation queues to run instructions
  // concurrently, use this to signal the backend which queue to dispatch to.
  // The backend should keep a mapping of
  // operation_queue_id->actual_hardware_queue_id if runtime will create
  // different IDs.
  int64 operation_queue_id = 1;

  // Specifies which operation queues to await for data when running with
  // multiple operation queues.
  repeated int64 wait_on_operation_queues = 2;

  // This attribute instructs the latency-hiding scheduler to
  // schedule this particular instruction to the earliest position.
  // Note that setting this to true will make this instruction scheduled
  // at the very beginning of the parent computation before
  // every other nodes.
  // An example use case would be deciding to schedule between collective
  // or an async compute. LHS might put either one at the first place
  // depending on the cost, but it'd be more beneficial if the collective
  // is always scheduled first as it's not SM-heavy.
  // In this case we can use this flag to enforce the ordering.
  bool force_earliest_schedule = 10;
}
