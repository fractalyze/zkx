/* Copyright 2017 The OpenXLA Authors.

Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at

    http://www.apache.org/licenses/LICENSE-2.0

Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
==============================================================================*/

syntax = "proto3";

package zkx;

option cc_enable_arenas = true;

// Primitive types are the individual values that can be held in rectangular
// multidimensional arrays. A description of the rectangular multidimensional
// array dimensions / primitive type is given by Shape, below.
enum PrimitiveType {
  // Invalid primitive type to serve as default.
  PRIMITIVE_TYPE_INVALID = 0;

  // Predicates are two-state booleans.
  PRED = 1;

  // Signed integral values of fixed width.
  S1 = 2;
  S2 = 3;
  S4 = 4;
  S8 = 5;
  S16 = 6;
  S32 = 7;
  S64 = 8;

  // Unsigned integral values of fixed width.
  U1 = 9;
  U2 = 10;
  U4 = 11;
  U8 = 12;
  U16 = 13;
  U32 = 14;
  U64 = 15;

  // A tuple is a polymorphic sequence; e.g. a shape that holds different
  // sub-shapes. They are used for things like returning multiple values from a
  // computation; e.g. a a bn254 G1 affine point can be represented as
  // tuple(bn254.sf, bn254.sf) for its (x, y) coordinates.
  //
  // If a shape proto has the tuple element type, it may not have any entries
  // in the dimensions field.
  TUPLE = 16;

  // An opaque type used for passing context-specific data to a custom
  // operation. Shapes of this primitive type will have empty dimensions and
  // tuple_shapes fields.
  //
  // (OPAQUE would be a better name for this identifier, but that conflicts with
  // a macro defined in windows.h.)
  OPAQUE_TYPE = 17;

  // A token type threaded between side-effecting operations. Shapes of this
  // primitive type will have empty dimensions and tuple_shapes fields.
  TOKEN = 18;

  BN254_SCALAR = 19;
  BN254_G1_AFFINE = 20;
  BN254_G1_JACOBIAN = 21;
  BN254_G1_XYZZ = 22;
  BN254_G2_AFFINE = 23;
  BN254_G2_JACOBIAN = 24;
  BN254_G2_XYZZ = 25;
}

// A DimLevelType indicates the encoding method for a dimension in an array.
// The semantics of this field are identical to those of the MLIR SparseTensor
// dialect.
// This should be kept in sync with the SparseTensor DimLevelType enum:
// https://github.com/llvm/llvm-project/blob/5674a3c/mlir/include/mlir/Dialect/SparseTensor/IR/SparseTensorAttrDefs.td#L86
enum DimLevelType {
  // The corresponding dimension is Dense, every entry is stored.
  DIM_DENSE = 0;
  // The corresponding dimension is Compressed, only nonzeros are stored.
  DIM_COMPRESSED = 1;
  // The corresponding dimension contains a single coordinate, no sibling
  // elements for each parent.
  DIM_SINGLETON = 2;
  // The corresponding dimension is Compressed, but with potential trailing
  // zeros, thus an extra upper bound (high) is used to exclude those zeros.
  // E.g., indices = [1, 2, 0, 0, 3, 4, 0, 0], position = [(0, 2), (4, 6)].
  DIM_LOOSE_COMPRESSED = 3;
}

// Describes a tile used in tiling-based layout. Refer to
// g3doc/third_party/xla/docs/tiled_layout.md for details about tiling-based
// layout.
message TileProto {
  // Number of elements in each dimension of the tile. It's ordered from the
  // most major dimension of the tile to the most minor dimension of the tile.
  // The dimensions correspond to a suffix of the dimensions of the shape being
  // tiled.
  repeated int64 dimensions = 1;
}

// Describes how data should be split between different memories.
message SplitConfigProto {
  // The dimension that is split.
  int64 dimension = 1;
  // The indices where each split point occurs. For example, if the dimension
  // size is 1024, a split_indices value of {512} indicates a two-way split of
  // data through the middle.
  repeated int64 split_indices = 2;
}

// A layout describes how the array is placed in (1D) memory space.  This
// includes the minor-to-major ordering of dimensions within a shape.
//
// Clients must specify the layouts of input Literals to the
// computation. Layouts specified in interior operations which take Shapes (for
// example, Convert) are ignored.
message LayoutProto {
  // The dimension level type list for this array, specifying the way in which
  // each array dimension is represented in memory. If this list is empty, the
  // array is assumed to be dense.
  repeated DimLevelType dim_level_types = 9;

  // Whether each dimension is unique or ordered.  Each of the following lists
  // must be empty, or have one entry for each entry of dim_level_types.  If
  // either list is empty, all dimensions are assumed to be unique and ordered,
  // respectively.  Entries in this list may not be false for some DimLevelType
  // values (such as DIM_DENSE in particular).
  repeated bool dim_unique = 13;
  repeated bool dim_ordered = 14;

  // Sequence of dimension numbers, from minor (fastest varying index) to major
  // (slowest varying index). This field is required.
  repeated int64 minor_to_major = 1;

  // A sequence of tiles, starting from the tile that's applied first to the
  // Shape.
  //
  // TODO(b/119839262): implement tiling in each backend or add Unimplemented
  // error.
  repeated TileProto tiles = 6;

  // The shape is padded at the end to multiple of, in terms of number of
  // elements. This is useful when tiling does not bring the shape to certain
  // desired granules. Tiling effectively pads/reshapes/transposes the shape
  // to another shape. This field pads the total number of elements of that
  // new shape to a multiple of certain number of elements. This is useful such
  // as we want a layout which does not tile the data but still requires it to
  // be padded to certain number of elements.
  int64 tail_padding_alignment_in_elements = 16;

  // (Optional) Bit size of each element. When unspecified or being 0, default
  // to ShapeUtil::ByteSizeOfPrimitiveType.
  int64 element_size_in_bits = 7;

  // Memory space where this array resides. The integer field is interpreted in
  // a backend-specific manner.
  int64 memory_space = 8;

  // The integer types to be used for indices and pointers.  These fields must
  // not be used unless the layout represents a sparse array.  The PrimitiveType
  // must correspond to an unsigned integer (U8, U16, U32, or U64).
  // If not provided, the compiler will use the largest unsigned integer
  // that is naturally supported by the target device (U32 or U64 in currently
  // supported devices).
  PrimitiveType index_primitive_type = 11;
  PrimitiveType pointer_primitive_type = 12;

  // The physical, on-device shape used to represent the shape this layout
  // belongs to. Only used for sparse arrays.
  // The layout(s) contained within the physical shape should not also contain
  // a physical shape.
  ShapeProto physical_shape = 10;

  // The dynamic shape metadata size in bytes in front of the shape data. The
  // field may be non-zero for a static shape whose associated buffer is for a
  // dynamic shape, e.g. a result of SliceToDynamic.
  int64 dynamic_shape_metadata_prefix_bytes = 15;

  // The split configurations which describe if/how the data is split between
  // different memories.
  repeated SplitConfigProto split_configs = 17;

  // The number of non-zero elements in the sparse tensor.
  int64 num_nonzeros = 10000;

  // Whether the elements are in Montgomery form.
  bool is_montgomery_form = 10001;

  // Important: if any field is added, be sure to modify ShapeUtil::Equal() and
  // LayoutUtil::Hash appropriately to account for the new field.

  reserved 2;
  reserved "padded_dimensions";
  reserved 3;
  reserved "padding_value";
  reserved 4;
  reserved "format";
  reserved 5;
  reserved "max_sparse_elements";
}

message ShapeProto {
  // TODO(chokobole): Remove these reserved fields.
  reserved 1;
  reserved "rank";

  // The element type for this shape.
  PrimitiveType element_type = 2;

  // The size (number of elements) for each dimension, or an upper bound on the
  // size if the dimension is dynamic.  In ZKX, dimensions are numbered from 0
  // to N-1 for an N-dimensional array. The first element of 'dimensions' is the
  // size of dimension 0, the second element is the size of dimension 1, and so
  // forth.  Empty list indicates a scalar.
  //
  // If the respective element in 'is_dimension_dynamic' is true then the value
  // in this field represents an upper bound on the size of the dimension.
  repeated int64 dimensions = 3;

  // For tuples only, the shapes of constituent shapes in the tuple sequence.
  repeated ShapeProto tuple_shapes = 4;

  // The layout used to back this shape.
  LayoutProto layout = 5;

  // For arrays, this indicates whether or not each dimension is
  // dynamically-sized. The number of elements in this repeated field should be
  // zero (indicating that no dimensions are dynamic) or equal to the number of
  // elements in the 'dimensions' field.
  repeated bool is_dynamic_dimension = 6;

  // Important: if any field is added, be sure to modify ShapeUtil::Equal(),
  // ShapeUtil::Compatible() and ShapeUtil::Hash() appropriately to account for
  // the new field.
}

// Shape of the parameters and output of a computation (like a traditional
// function signature).
message ProgramShapeProto {
  repeated ShapeProto parameters = 1;
  ShapeProto result = 2;
  repeated string parameter_names = 3;
}

// The type optimization profiles in use for Op-level optimizations.
enum ProfileType {
  INVALID = 0;
  WINDOW = 1;
  FLAG = 2;
  INTEGER = 3;
}

// The source of the optimization profile.
enum ProfileSource {
  PROFILE_SOURCE_UNKNOWN_SOURCE = 0;
  PROFILE_SOURCE_EMBEDDED = 1;
  PROFILE_SOURCE_REMOTE = 2;
}

// The compilation event that triggered the use of the profile.
enum CompilationEvent {
  COMPILATION_EVENT_UNKNOWN_EVENT = 0;
  COMPILATION_EVENT_FIRST_COMPILATION = 1;
  COMPILATION_EVENT_RECOMPILATION = 2;
}

// Symbolization metadata for HLO Instructions.
//
// This metadata is used for debugging ZKX code generation, as well as
// performance profiling of ZKX-generated executables.
message OpMetadata {
  // The framework op name that generated this ZKX op.
  //
  // TODO(chokobole): Replace SoftMax with the appropriate op_type.
  // Frameworks that build on top of ZKX should mirror the names of their ops
  // back to users by specifying the op_type. In this way, even if the
  // framework's "ops" are implemented as multiple ZKX HLO Ops, they can be
  // grouped appropriately. (e.g. if a SoftMax layer is emitted into ZKX as
  // multiple ops, then each op should have the op_type be "SoftMax".)
  string op_type = 1;
  // The user-specified name of the op.
  //
  // This name is often unique within a computation. Note: some frameworks
  // add auto-generated names if the user does not provide one.
  string op_name = 2;
  // Indicate a file and line that this op is associated to in a user's program.
  //
  // e.g. it could be the file and line of user code that generated the op.
  string source_file = 3;
  int32 source_line = 4;

  reserved 5;
  reserved "creation_pass_id";

  reserved 6;
  reserved "logical_creation_pass_id";

  // The footprint of the generated code for the instruction.
  int64 size_of_generated_code_in_bytes = 7;
  // The size of the working set, i.e., the amount of memory, used by the
  // instruction in a compiler-managed fast device memory.
  int64 size_of_memory_working_set_in_bytes = 8;

  // Information about the optimization profile that this operation contains.
  message ProfileInfo {
    // The type of optimization profiles that this operation contains.
    repeated ProfileType profile_type = 1;
    // Speedup of tuned config compared to default config.
    // TODO(b/203817882) Set the relative_speedup.
    double relative_speedup = 2;
    // The source of the optimization profiles that this operation contains.
    ProfileSource profile_source = 3;
    // The compilation event that triggered the use of the profiles.
    CompilationEvent compilation_event = 4;
  }

  // Profile information for the Op.
  ProfileInfo profile_info = 9;

  reserved 10;

  // Deduplicated HLO name for this op. In some cases, we can have multiple
  // instructions (e.g. fusions) that are considered duplicates. We want to
  // group them together under the same name so that we can group them together
  // during analysis (e.g. HLO Op Profile tool in Xprof).
  // E.g. If we have fusion.1, fusion.2, and fusion.3 marked as duplicates,
  // fusion.2 and fusion.3 will have deduplicated_name = fusion.1
  string deduplicated_name = 11;

  reserved 12;

  reserved 13;

  // 1-based position of the frame in frames flat array.
  // Ids are 1-based to keep 0 value as representation of non-set property.
  int32 stack_frame_id = 14;

  // Instruction name available upon scheduling.
  string scheduling_name = 15;
}

// Profile data from the execution of a computation.
message ExecutionProfile {
  // Whether the executable was read from the compilation cache.
  bool compilation_cache_hit = 1;

  // The time in milliseconds spent to compile the computation. This only set if
  // the executable was not read from the compilation cache
  // (compilation_cache_hit == false).
  int64 compile_time_ms = 2;

  // The number of cycles spent for the computation. This does not include the
  // time taken for the data transfers between the host and the device. This is
  // a target-dependent field and only used for debugging purposes.
  int64 compute_cycle_count = 3;

  // The time in nanoseconds spent for the computation, without data transfer.
  int64 compute_time_ns = 4;

  // The time in nanoseconds spent for the entire computation, including the
  // result data transfer time. Current implementation does not spend any cycles
  // for the input data transfer since the memory is initialized with the proper
  // values before the execution.
  int64 compute_and_transfer_time_ns = 5;

  // The size of the binary code in the executable.
  int64 executable_size_in_bytes = 6;

  // Whether this profile was drawn from a cache of profiles instead of from
  // execution on the hardware.
  bool profile_cache_hit = 7;

  // Whether a warm-up run of the computation was executed before the
  // measured execution.
  bool warmup_run_executed = 8;
}

// Handle given to a user that represents an execution that the user launched
// asynchronously on the device.
message ExecutionHandle {
  int64 handle = 1;
}

// Handle given to a user that represents a globally accessible allocation.
// Contrast this against a ComputationDataHandle, which is not globally
// accessible, since it only exists within a specific computation.
message GlobalDataHandle {
  int64 handle = 1;
}

// Handle given to a user that represents a replicated virtual device. Each
// replicated device represents N physical devices for execution where N is the
// number of replicas.
message DeviceHandle {
  int64 handle = 1;

  // The number of model-parallel virtual devices that communicate via ZKX
  // Send/Recv instructions.
  int64 device_count = 2;
}

// Handle given to a user to represent a channel between two computations
// via a Send and Recv instruction pair. Channels are unbuffered, so Send
// Send instructions will be blocked until the data is transferred.
message ChannelHandle {
  int64 handle = 1;
  enum ChannelType {
    // Invalid primitive type to serve as default.
    CHANNEL_TYPE_INVALID = 0;

    // A channel for sending data between devices.
    DEVICE_TO_DEVICE = 1;

    // A channel for sending data from the device to the host. Can only be used
    // with a Send operation.
    DEVICE_TO_HOST = 2;

    // A channel for sending data from the host to the device. Can only be used
    // with a Recv operation.
    HOST_TO_DEVICE = 3;
  }
  ChannelType type = 2;
}

// DeviceAssignmentProto is a serialized form of DeviceAssignment class, which
// represents the device ids assigned to a set of replicated computations.
// See zkx::DeviceAssignment class comment for more details.
message DeviceAssignmentProto {
  int32 replica_count = 1;
  int32 computation_count = 2;

  // Each logical computation runs on replica_count physical devices.
  // ComputationDevice represents the device ids assigned to the replicas.
  message ComputationDevice {
    repeated int64 replica_device_ids = 1;
  }
  repeated ComputationDevice computation_devices = 3;
}

// Literals are used when the server and client need to exchange materialized
// data / results. Literals are also used to describe constants used in
// computations.
//
// Transfers to/from the client are encoded in literal form, and the structure
// of the repeated fields is implied by the shape.
message LiteralProto {
  ShapeProto shape = 1;
  repeated bool preds = 2;
  bytes s1s = 3;
  bytes s2s = 4;
  bytes s4s = 5;
  bytes s8s = 6;
  bytes u1s = 7;
  bytes u2s = 8;
  bytes u4s = 9;
  bytes u8s = 10;
  // The U16s and S16s are encoded in little endian byte order.
  bytes u16s = 11;
  bytes s16s = 12;
  repeated int32 s32s = 13;
  repeated int64 s64s = 14;
  repeated uint32 u32s = 15;
  repeated uint64 u64s = 16;
  repeated LiteralProto tuple_literals = 17;
  repeated int64 sparse_indices = 18;

  bytes bn254_scalars = 19;
  bytes bn254_g1_affines = 20;
  bytes bn254_g1_jacobians = 21;
  bytes bn254_g1_xyzzs = 22;
  bytes bn254_g2_affines = 23;
  bytes bn254_g2_jacobians = 24;
  bytes bn254_g2_xyzzs = 25;
}

enum FftType {
  // Forward FFT
  FFT = 0;
  // Inverse FFT
  IFFT = 1;
}

message DotDimensionNumbers {
  // The dimension numbers that represent the 'lhs' contracting dimensions.
  repeated int64 lhs_contracting_dimensions = 1;
  // The dimension numbers that represent the 'rhs' contracting dimensions.
  repeated int64 rhs_contracting_dimensions = 2;
  // The dimension numbers that represent the 'lhs' batch dimensions.
  repeated int64 lhs_batch_dimensions = 3;
  // The dimension numbers that represent the 'rhs' batch dimensions.
  repeated int64 rhs_batch_dimensions = 4;
}

enum SparsityType {
  SPARSITY_INVALID = 0;

  // Structured N:M sparsity.
  SPARSITY_STRUCTURED_N_M = 1;

  // Next: 2
}

// Contains sparsity metadata for a sparse dot operation.
// The only supported type atm is structured 2:4 sparsity, which is natively
// supported on NVidia GPUs.
// Restrictions:
// - only one operand of the dot operation may be sparse;
// - only the contracting dimension may be sparse.
message SparsityDescriptor {
  SparsityType type = 1;

  // Sparse operand index (0 or 1).
  int32 index = 2;
  // Sparse dimension number.
  int32 dimension = 3;

  // Structured N:M sparsity (N < M).
  int32 n = 4;
  int32 m = 5;

  // Next: 6
}

// Generic map of attributes used to pass hints / configuration options from
// the Python frontend to the XLA backend.
message FrontendAttributes {
  map<string, string> map = 1;
}

// Represents a single statistic to track.
message Statistic {
  // Must be a single word consisting of any alphanumeric characters
  string stat_name = 1;
  // Must be within a range of [0, 100], in order for the graph dumper to
  // properly render the statistic onto the graph.
  double stat_val = 2;
}

// Represents the information needed to visualize propagation statistics when
// rendering an HLO graph. This includes an array of statistics as well as the
// index of the statistic to render.
message StatisticsViz {
  int64 stat_index_to_visualize = 1;
  repeated Statistic statistics = 2;
}

message OpSharding {
  enum Type {
    // This sharding is replicated across all devices (implies maximal,
    // all other fields are unused).
    REPLICATED = 0;
    // This sharding is maximal - one device runs the entire operation.
    MAXIMAL = 1;
    // This sharding is a tuple - only the tuple_shardings field is valid.
    TUPLE = 2;
    // None of the above; tile_shape and tile_assignment are both used.
    OTHER = 3;
    // This op is manually sharded: the shapes are already partitioned and the
    // partitioner should not change this op.
    MANUAL = 4;
    // This sharding is a placeholder sharding with lowest precedence, it can be
    // overwritten by any other shardings.
    UNKNOWN = 5;
  }
  Type type = 1;
  // The shape of the sharded tile.
  ShapeProto tile_shape = 2;
  // The shape of the tile assignment tensor - this must be the same rank as
  // tile_shape and the product of its dimensions must equal
  // tile_assignment_devices.size().
  repeated int64 tile_assignment_dimensions = 3;
  // Flattened list of device IDs. The order of flattening is the same as used
  // by IndexUtil::MultiToLinearIndex(tile_assignment_shape).
  // Only one of tile_assignment_devices and iota_dimensions shall be non-empty.
  repeated int64 tile_assignment_devices = 4;
  // If type == TUPLE, the sub-shardings, one per leaf node in the tuple shape,
  // in pre-order. The tuple shape could be nested; here we store just a
  // flattened list of all leaves in the tuple shape. Note that the tuple shape
  // is not stored here; shardings do not store the shapes to which they are
  // applied, this is inferred from the instruction this sharding gets attached
  // to.
  repeated OpSharding tuple_shardings = 5;

  // Only used for OTHER type. If true, data is sharded according to other
  // dimensions of tile_assignment(), but replicated across devices along the
  // last dimension. (Experimental)
  bool replicate_on_last_tile_dim = 6;
  // This field is used to track the source of this sharding, usually derived
  // from instructions. Multiple metadata may be populated if sharding is
  // combined with other shardings.  Metadata are to not be populated when
  // type == TUPLE and instead metadata should be set on individual tuple
  // elements.
  repeated OpMetadata metadata = 7;

  // This field is used to represented the sharding type of each subgroup.
  // For example, sharding={devices=[2,2,2,2]0,1,2,...,15 last_tile_dims={
  // replicate, manual, unreduced}} means that each of the last 3 dimensions
  // in [2,2,2,2] represents a subgrouping in replicate, manual,
  // unreduced sharding type respectively.
  repeated Type last_tile_dims = 8;

  // Dimensions used to reshape the 1D iota array of device IDs.
  // Only one of tile_assignment_devices and iota_reshape_dims shall be
  // non-empty.
  repeated int64 iota_reshape_dims = 9;

  // Dimension permutations to transposed the iota array reshaped to
  // iota_reshape_dims. This must have the same size as iota_reshape_dims.
  repeated int32 iota_transpose_perm = 10;

  // This field decides whether this op is in a shard group.
  bool is_shard_group = 11;

  // This field is used to store the unique id of the shard group.
  int64 shard_group_id = 12;

  // Used to decide whether this op is to be sharded like some other ops, or to
  // which other ops will be sharded like.
  enum ShardGroupType {
    // This op will be sharded exactly the same as the other op. (hard
    // restriction)
    AS = 0;
    // This op will try to allow sharding propagation within the same group even
    // there is no data dependencies among them, but there is no guarantee that
    // the final shardings within the same group will be exactly the same. (soft
    // restriction)
    LIKE = 1;
  }

  ShardGroupType shard_group_type = 13;
}

// Describes the replica groups in a cross replica op (e.g., all-reduce and
// all-to-all).
message ReplicaGroup {
  // The ids of the replicas that belongs to the same group. The ordering of the
  // ids matters in some ops (e.g., all-to-all).
  repeated int64 replica_ids = 1;
}

// Represents a list of replica groups (a list of list of devices) with
// reshaping and transposing an iota array (iota tile assignment). Can be used
// to represent certain common patterns of device lists in a compact, scalable
// format.
message IotaReplicaGroupListProto {
  // Number of replica groups.
  int64 num_replica_groups = 1;

  // Number of devices per group.
  int64 num_devices_per_group = 2;

  // The dimensions used to reshape the 1D iota array of device IDs.
  repeated int64 iota_reshape_dims = 3;

  // The dimension permutations to transposed the iota array reshaped to
  // iota_reshape_dims. This must have the same size as iota_reshape_dims.
  repeated int32 iota_transpose_perm = 4;
}

// Represents a series of devices participating in a collective operation (e.g.,
// all-reduce and all-to-all). While this directly translates to a list of
// replica groups, it may be used to represent these lists in a compact form.
message CollectiveDeviceListProto {
  // ReplicaGroupV1: List of replica groups. Legacy way of representing device
  // lists.
  repeated ReplicaGroup replica_groups = 1;

  // ReplicaGroupV2: Represents a list of replica groups with reshaping and
  // transposing an iota array.
  IotaReplicaGroupListProto iota_replica_group_list = 2;
}

// Describes the source target pair in the collective permute op.
message SourceTarget {
  int64 source = 1;
  int64 target = 2;
}

// Describes whether all data-parallelism replicas will receive the same
// parameter data at each buffer.
message ParameterReplication {
  // A list of boolean values for the flattened leaf buffers. Each value
  // indicates whether the corresponding leaf buffer is replicated.
  //
  // If this field is empty, it means no buffer is replicated. Otherwise, the
  // number of elements in this field must match the number of leaf buffers in
  // the HLO instruction's shape.
  repeated bool replicated_at_leaf_buffers = 1;
}

// Specifies a pair of output/operand buffers that alias each other for
// kCustomCall and kFusion
message OutputOperandAliasing {
  repeated int64 output_shape_index = 1;
  int64 operand_index = 2;
  repeated int64 operand_shape_index = 3;
}

message OriginalArrayProto {
  repeated int64 leaf_shape_index = 1;
  string instruction_name = 2;
  repeated int64 shape_index = 3;
}

message OriginalValueProto {
  repeated OriginalArrayProto leaves = 1;
}
